{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a File\n",
    "\n",
    "Read the content of the file `files/poem.txt` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The road not taken\n",
      "By Robert Frost\n",
      "\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('files/poem.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to a File\n",
    "\n",
    "Write a note to the `files/note.txt` file that says, \"Remember to buy milk and eggs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/note.txt', 'w') as file:\n",
    "    file.write(\"Remember to buy milk and eggs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append to a File\n",
    "\n",
    "Append a new task to the previous `note.txt` file in a new line: \"And Beer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/note.txt', 'a') as file:\n",
    "    file.write(\"\\nAnd Beer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to buy milk and eggs.\n",
      "And Beer!"
     ]
    }
   ],
   "source": [
    "!cat files/note.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Lines\n",
    "\n",
    "Count the number of times \"apple\" appears in the `files/fruits.txt` file and print the count (it should be 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'apple' appears 5 times.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open('files/fruits.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if 'apple' in line.strip():\n",
    "            count += 1\n",
    "print(f\"'apple' appears {count} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy\n",
    "\n",
    "Copy the content from `files/source.txt` to the new file `files/destination.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/source.txt', 'r') as source:\n",
    "    content = source.read()\n",
    "    with open('files/destination.txt', 'w') as destination:\n",
    "        destination.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With CSV\n",
    "\n",
    "Open the `files/grades.csv` file, calculate the average grade for each student, and print their names along with their average grades.\n",
    "\n",
    "Solve this exercise in 3 different ways:\n",
    "- Read the file in plain text (without using any CSV module).\n",
    "- Use `csv` module.\n",
    "- Use `pandas` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's average grade is: 87.67\n",
      "Bob's average grade is: 85.33\n",
      "Charlie's average grade is: 85.00\n",
      "John's average grade is: 60.33\n",
      "Amy's average grade is: 55.33\n",
      "Hector's average grade is: 76.33\n",
      "Mark's average grade is: 34.00\n"
     ]
    }
   ],
   "source": [
    "# Plain text\n",
    "\n",
    "averages = {}\n",
    "\n",
    "with open('files/grades.csv', 'r') as file:\n",
    "    lines = file.readlines()[1:]  # Skip the header line\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split(',')\n",
    "        name = parts[0]\n",
    "        grades = [int(x) for x in parts[1:]]\n",
    "        avg = sum(grades) / len(grades)\n",
    "        averages[name] = avg\n",
    "\n",
    "for name, avg in averages.items():\n",
    "    print(f\"{name}'s average grade is: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's average grade is: 87.67\n",
      "Bob's average grade is: 85.33\n",
      "Charlie's average grade is: 85.00\n",
      "John's average grade is: 60.33\n",
      "Amy's average grade is: 55.33\n",
      "Hector's average grade is: 76.33\n",
      "Mark's average grade is: 34.00\n"
     ]
    }
   ],
   "source": [
    "# csv\n",
    "import csv\n",
    "\n",
    "averages = {}\n",
    "\n",
    "with open('files/grades.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        name = row['Name']\n",
    "        grades = [int(row['Math']), int(row['Science']), int(row['English'])]\n",
    "        avg = sum(grades) / len(grades)\n",
    "        averages[name] = avg\n",
    "\n",
    "for name, avg in averages.items():\n",
    "    print(f\"{name}'s average grade is: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('files/grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>87.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>85.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John</td>\n",
       "      <td>60.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amy</td>\n",
       "      <td>55.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hector</td>\n",
       "      <td>76.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mark</td>\n",
       "      <td>34.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name   Mean\n",
       "0    Alice  87.67\n",
       "1      Bob  85.33\n",
       "2  Charlie  85.00\n",
       "3     John  60.33\n",
       "4      Amy  55.33\n",
       "5   Hector  76.33\n",
       "6     Mark  34.00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df['Name'], df.iloc[:, 1:].mean(axis=1).round(2)], axis=1, keys=['Name', 'Mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple\n",
    "\n",
    "Given a JSON file `files/simple.json`, add a new key-value pair \"Language\": \"Python\" and write the modified data back to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('files/simple.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data['Language'] = 'Python'\n",
    "\n",
    "with open('files/simple.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Name\": \"John\",\n",
      "    \"Age\": 25,\n",
      "    \"City\": \"New York\",\n",
      "    \"Language\": \"Python\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat files/simple.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Display\n",
    "\n",
    "Read data from the `files/info.yml` YAML file and display the content in the follwing order: `name`, `age`, and `hobbies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice\n",
      "Age: 30\n",
      "Hobbies:\n",
      "- reading\n",
      "- hiking\n",
      "- swimming\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('files/info.yml', 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Name: {data['person']['name']}\")\n",
    "print(f\"Age: {data['person']['age']}\")\n",
    "print(\"Hobbies:\")\n",
    "for hobby in data['person']['hobbies']:\n",
    "    print(f\"- {hobby}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exception Handling\n",
    "\n",
    "Try to open `files/missing.txt`. If there's an error related to the file not existing, print \"The file does not exist.\" and set the contents of the file to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file does not exist.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('files/missing.txt', 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "except FileNotFoundError:\n",
    "    print(\"The file does not exist.\")\n",
    "    content = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "\n",
    "Calculate the frequency of each word in the `files/bigtext.txt` file and determine the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2),\n",
       " ('long', 2),\n",
       " ('feel', 1),\n",
       " ('free', 1),\n",
       " ('to', 1),\n",
       " ('use', 1),\n",
       " ('an', 1),\n",
       " ('excerpt', 1),\n",
       " ('from', 1),\n",
       " ('book', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "with open('files/bigtext.txt', 'r') as f:\n",
    "    text = f.read().lower()\n",
    "    # Removing punctuation and splitting by whitespace\n",
    "    words = [word.strip('.,!?()[]{}\":;') for word in text.split()]\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "# Sort the dictionary by value to get the top 10 words\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10 = sorted_words[:10]\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Parsing\n",
    "\n",
    "List the IDs of users who have more than 5 incomplete tasks in the `files/todos.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 has 9 incomplete tasks\n",
      "User 2 has 12 incomplete tasks\n",
      "User 3 has 13 incomplete tasks\n",
      "User 4 has 14 incomplete tasks\n",
      "User 5 has 8 incomplete tasks\n",
      "User 6 has 14 incomplete tasks\n",
      "User 7 has 11 incomplete tasks\n",
      "User 8 has 9 incomplete tasks\n",
      "User 9 has 12 incomplete tasks\n",
      "User 10 has 8 incomplete tasks\n"
     ]
    }
   ],
   "source": [
    "with open('files/todos.json', 'r') as f:\n",
    "    todos = json.load(f)\n",
    "\n",
    "# Count incomplete tasks for each user\n",
    "incomplete_counts = {}\n",
    "for todo in todos:\n",
    "    if not todo['completed']:\n",
    "        if todo['userId'] in incomplete_counts:\n",
    "            incomplete_counts[todo['userId']] += 1\n",
    "        else:\n",
    "            incomplete_counts[todo['userId']] = 1\n",
    "\n",
    "for userId, count in incomplete_counts.items():\n",
    "    if count > 5:\n",
    "        print(f'User {userId} has {count} incomplete tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Analyzer\n",
    "\n",
    "Given a log file `files/logs.txt` with entries of the form `[Timestamp] [LOG LEVEL] Message`, extract and count the number of occurrences of each log level (e.g., INFO, WARN, ERROR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_log(file_path):\n",
    "    log_levels = {\"INFO\": 0, \"WARN\": 0, \"ERROR\": 0}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            for level in log_levels:\n",
    "                if level in line:\n",
    "                    log_levels[level] += 1\n",
    "    return log_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 12\n",
      "WARN: 4\n",
      "ERROR: 6\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "log_counts = analyze_log('files/logs.txt')\n",
    "for level, count in log_counts.items():\n",
    "    print(f\"{level}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts\n",
    "\n",
    "In the following exercises use the files:\n",
    "- `files/users.json`\n",
    "- `files/posts.json`\n",
    "- `files/comments.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open('files/users.json', 'r') as f:\n",
    "    users = json.load(f)\n",
    "with open('files/posts.json', 'r') as f:\n",
    "    posts = json.load(f)\n",
    "with open('files/comments.json', 'r') as f:\n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Data\n",
    "\n",
    "Find and print the `postId` values that are present in both the posts and the comments in `files/posts.json` and `files/comments.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_ids = {post['id'] for post in posts}\n",
    "comment_post_ids = {comment['postId'] for comment in comments}\n",
    "\n",
    "# Common post IDs\n",
    "post_ids.intersection(comment_post_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Activity Logs\n",
    "\n",
    "- Read `files/users.json` and write each user's name and email to a new `files/users/users_names.txt` file.\n",
    "- Read `files/posts.json` and segregate them by users. Write each user's posts to a separate file named after them: `files/users/{user_id}_{user_name}.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "users_directory = 'files/users/'\n",
    "if not os.path.exists(users_directory):\n",
    "    os.mkdir(users_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write user names and emails to users.txt\n",
    "file_path = os.path.join(users_directory, 'user_names.txt')\n",
    "with open(file_path, 'w') as users_file:\n",
    "    for user in users:\n",
    "        users_file.write(f\"Name: {user['name']}, Email: {user['email']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/posts.json', 'r') as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "# Segregate posts by users and write to respective files\n",
    "for user in users:\n",
    "    user_posts = [post for post in posts if post['userId'] == user['id']]\n",
    "\n",
    "    file_name = f\"{user['id']}_{user['name'].replace(' ', '_')}.txt\"\n",
    "    file_path = os.path.join(users_directory, file_name)\n",
    "    with open(file_path, 'w') as post_file:\n",
    "        for post in user_posts:\n",
    "            post_file.write(f\"Title: {post['title']}\\n\")\n",
    "            post_file.write(f\"Body: {post['body']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Length Analysis\n",
    "\n",
    "Analyze the posts and create a file, `post_length_analysis.txt`, that categorizes posts based on their length: \"Short\" (0-50 chars), \"Medium\" (51-200 chars), and \"Long\" (>200 chars). List the number of posts in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_posts = 0\n",
    "medium_posts = 0\n",
    "long_posts = 0\n",
    "\n",
    "for post in posts:\n",
    "    length = len(post['body'])\n",
    "    if length <= 50:\n",
    "        short_posts += 1\n",
    "    elif 50 < length <= 200:\n",
    "        medium_posts += 1\n",
    "    else:\n",
    "        long_posts += 1\n",
    "\n",
    "with open('files/users/post_length_analysis.txt', 'w') as file:\n",
    "    file.write(f\"Short posts: {short_posts}\\n\")\n",
    "    file.write(f\"Medium posts: {medium_posts}\\n\")\n",
    "    file.write(f\"Long posts: {long_posts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Domain Counter\n",
    "\n",
    "Analyze the email addresses of all users and list the frequency of each email domain (e.g., @example.com) in a file called `email_domains.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "email_domains = defaultdict(int)\n",
    "\n",
    "for user in users:\n",
    "    domain = user['email'].split('@')[1]\n",
    "    email_domains[domain] += 1\n",
    "\n",
    "with open('files/users/email_domains.txt', 'w') as file:\n",
    "    for domain, count in email_domains.items():\n",
    "        file.write(f\"{domain}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Engaged Users\n",
    "\n",
    "Identify the top 3 users who've made the most posts and received the highest number of comments on their posts. Generate a `most_engaged_users.json` that contains the user details, total posts, and total comments received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagement = []\n",
    "\n",
    "for user in users:\n",
    "    user_posts = [post for post in posts if post['userId'] == user['id']]\n",
    "    post_ids = [post['id'] for post in user_posts]\n",
    "    total_comments = sum(1 for comment in comments if comment['postId'] in post_ids)\n",
    "    user_engagement.append({\n",
    "        'User': user['name'],\n",
    "        'Posts': len(user_posts),\n",
    "        'Comments': total_comments\n",
    "    })\n",
    "\n",
    "pd.DataFrame(user_engagement)\\\n",
    "    .sort_values(by=['Comments', 'Posts'], ascending=False)\\\n",
    "    .head(3)\\\n",
    "    .to_json('files/users/most_engaged_users.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-Post Analysis\n",
    "\n",
    "Analyze where posts are coming from geographically. For each city where users reside, calculate the average number of posts made. Generate `city_post_avg.json` containing each city and its average post count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_post_count = {}\n",
    "\n",
    "for user in users:\n",
    "    user_posts = [post for post in posts if post['userId'] == user['id']]\n",
    "    if user['address']['city'] in city_post_count:\n",
    "        city_post_count[user['address']['city']] += len(user_posts)\n",
    "    else:\n",
    "        city_post_count[user['address']['city']] = len(user_posts)\n",
    "\n",
    "avg_post_count = {city: post_count/len(users) for city, post_count in city_post_count.items()}\n",
    "\n",
    "with open('files/users/city_post_avg.json', 'w') as file:\n",
    "    json.dump(avg_post_count, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Interconnectivity\n",
    "\n",
    "Check if users mention other users in their posts (based on usernames). Generate a `user_mentions.json` file that lists for each user which other users they've mentioned the most across all their posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = {}\n",
    "\n",
    "for user in users:\n",
    "    mentions[user['username']] = {}\n",
    "    user_posts = [post for post in posts if post['userId'] == user['id']]\n",
    "    for post in user_posts:\n",
    "        for other_user in users:\n",
    "            if other_user['username'] in post['body'] and other_user['username'] != user['username']:\n",
    "                mentions[user['username']].setdefault(other_user['username'], 0)\n",
    "                mentions[user['username']][other_user['username']] += 1\n",
    "\n",
    "most_mentions = {}\n",
    "for user, mention_count in mentions.items():\n",
    "    sorted_mentions = sorted(mention_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    if sorted_mentions:\n",
    "        most_mentions[user] = sorted_mentions[0]\n",
    "\n",
    "with open('files/users/user_mentions.json', 'w') as file:\n",
    "    json.dump(most_mentions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Comparison Tool\n",
    "\n",
    "Create a tool that takes in the paths of two text files and outputs the lines that differ between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_files(file1, file2):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        lines1 = f1.readlines()\n",
    "        lines2 = f2.readlines()\n",
    "\n",
    "    differences = []\n",
    "\n",
    "    for l1, l2 in zip(lines1, lines2):\n",
    "        if l1 != l2:\n",
    "            differences.append((l1, l2))\n",
    "\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage (no differences expected):\n",
    "differences = compare_files('files/users/1_Leanne_Graham.txt', 'files/users/1_Leanne_Graham.txt')\n",
    "for diff in differences:\n",
    "    print(f\"File 1: {diff[0]}File 2: {diff[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  File Metadata Extractor\n",
    "\n",
    "Create a Python program that navigates through every file and folder in a given directory (recursively), extracts metadata from each file, and then saves this metadata to a CSV file. The metadata to capture for each file: File Name, File Path, File Size, Last Modified Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "def extract_metadata(directory, csv_file):\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"File Name\", \"File Path\", \"File Size (Bytes)\", \"Last Modified Date\"])\n",
    "        \n",
    "        for foldername, subfolders, filenames in os.walk(directory):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(foldername, filename)\n",
    "                filesize = os.path.getsize(filepath)\n",
    "                timestamp = os.path.getmtime(filepath)\n",
    "                # Convert timestamp to datetime object\n",
    "                dt_object = datetime.datetime.fromtimestamp(timestamp)\n",
    "                # Convert datetime object to string\n",
    "                last_modified = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                writer.writerow([filename, filepath, filesize, last_modified])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "extract_metadata('files/', 'files/metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Overflow Survey Analysis\n",
    "\n",
    "Unzip the datasets contained in `files/stackoverflow/`.\n",
    "\n",
    "These datasets contain responses to the annual Stack Overflow survey, including various aspects like the type of developer, education, job satisfaction, and more.\n",
    "\n",
    "**Objective**:\n",
    "Analyze the Stack Overflow Developer Survey data to gain insights into the development community and its trends. This will involve integrating the yearly datasets, extracting relevant insights, visualizing the results, and creating a comprehensive PDF report.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "\n",
    "1. File Discovery & Data Exploration:\n",
    "    - Download and unzip the dataset folder.\n",
    "    - Programmatically discover and read the yearly survey files.\n",
    "    - Familiarize yourself with the structure and columns in the datasets.\n",
    "\n",
    "2. Data Integration & Cleaning:\n",
    "    - Handle missing values and inconsistencies between yearly datasets.\n",
    "    - Integrate data for the past 5 years into one consolidated dataset, taking care of column differences and inconsistencies.\n",
    "\n",
    "3. Analysis:\n",
    "    - Identify the top 5 most popular programming languages for the past 5 years.\n",
    "    - Calculate the median salary for developers in the US, Europe, and Asia for each year.\n",
    "    - Determine the percentage of remote workers over the years.\n",
    "\n",
    "4. Visualizations:\n",
    "    - Plot the trends of the top 5 programming languages over the past 5 years.\n",
    "    - Create a bar chart comparing the median salaries in the US, Europe, and Asia for each year.\n",
    "    - Generate a pie chart showing the distribution of developers based on their highest level of formal education.\n",
    "\n",
    "5. Generate PDF Report:\n",
    "    - Create a comprehensive report detailing the above findings.\n",
    "    - Include the charts generated in the visualizations step.\n",
    "    - Save the report as a StackOverflow_Survey_Analysis.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
